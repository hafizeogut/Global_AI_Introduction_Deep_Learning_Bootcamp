{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYmYuGSWFs3-"
   },
   "source": [
    "# Building a Recurrent Neural Network\n",
    "\n",
    "## Sentiment Analysis\n",
    "In this project, we will build a Long Short-term Memory (LSTM) neural network to solve a binary sentiment analysis problem.\n",
    "\n",
    "For this, we'll use the â€œIMDB Movie Review Dataset\" available on Keras. It includes 50000 highly polarized movie reviews categorized as positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "# Tekrarlayan Bir Sinir AÄŸÄ± OluÅŸturmak\n",
    "\n",
    "## Duygu Analizi\n",
    "Bu projede, ikili duygu analizi problemini Ã§Ã¶zmek iÃ§in Uzun KÄ±sa SÃ¼reli Bellek (LSTM) sinir aÄŸÄ± oluÅŸturacaÄŸÄ±z.\n",
    "\n",
    "Bunun iÃ§in Keras'ta bulunan â€œIMDB Film Ä°nceleme Veri KÃ¼mesiniâ€ kullanacaÄŸÄ±z. Olumlu veya olumsuz olarak kategorize edilmiÅŸ 50.000 yÃ¼ksek derecede kutuplaÅŸmÄ±ÅŸ film incelemesini iÃ§erir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQ7xy4lzfsw"
   },
   "source": [
    "## Importing the required libraries\n",
    "We'll start with importing required libraries.\n",
    "\n",
    "ğŸ“Œ Use the keyword \"import\".\n",
    "\n",
    "\n",
    "## Gerekli kitaplÄ±klarÄ± iÃ§e aktarma\n",
    "Gerekli kÃ¼tÃ¼phaneleri iÃ§e aktararak baÅŸlayacaÄŸÄ±z.\n",
    "\n",
    "ğŸ“Œ \"Ä°Ã§e aktar\" anahtar kelimesini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b__mue-XGPZ9"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import NumPy and Matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0b5YzGHP3qs"
   },
   "source": [
    "## Dataset\n",
    "Let's download the IMDB dataset which is included in Keras, and assign it to the corresponding variables *X_train*, *y_train*, *X_test*, and *y_test*. We want to include the most frequently used 10000 words, so we specify 10000 for the num_words parameter.\n",
    "\n",
    "ğŸ“Œ Use the datasets.imdb.load_data() function of the Keras.\n",
    "\n",
    "## Veri KÃ¼mesi\n",
    "Keras'Ä±n iÃ§erdiÄŸi IMDB veri setini indirelim ve onu ilgili *X_train*, *y_train*, *X_test* ve *y_test* deÄŸiÅŸkenlerine atayalÄ±m. En sÄ±k kullanÄ±lan 10000 kelimeyi dahil etmek istiyoruz bu yÃ¼zden num_words parametresi iÃ§in 10000 belirtiyoruz.\n",
    "\n",
    "ğŸ“Œ Keras'Ä±n datasets.imdb.load_data() fonksiyonunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1WLgLQxGGDz8"
   },
   "outputs": [],
   "source": [
    "# Download the IMDB dataset included in Keras\n",
    "# Set the parameter num_words to 10000\n",
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUPnNCgC0mHm"
   },
   "source": [
    "Before we move on, we can print a single sample to see what the data looks like.\n",
    "\n",
    "ğŸ“Œ Use the print() function for this.\n",
    "\n",
    "Devam etmeden Ã¶nce verilerin neye benzediÄŸini gÃ¶rmek iÃ§in tek bir Ã¶rnek yazdÄ±rabiliriz.\n",
    "\n",
    "ğŸ“Œ Bunun iÃ§in print() fonksiyonunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1spB5eY9xh-B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKkhznIa8hIw"
   },
   "source": [
    "Then, we print the the number of samples in the X_train and X_test datasets to see how the dataset is distributed.\n",
    "\n",
    "ğŸ“Œ Use f-strings for this.\n",
    "\n",
    "Daha sonra veri setinin nasÄ±l daÄŸÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶rmek iÃ§in X_train ve X_test veri setlerindeki Ã¶rnek sayÄ±sÄ±nÄ± yazdÄ±rÄ±yoruz.\n",
    "\n",
    "ğŸ“ŒBunun iÃ§in f-stringleri kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "skzb2oTCdV-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:25000\n",
      "X_test:25000\n"
     ]
    }
   ],
   "source": [
    "# Print the number of samples\n",
    "print (f\"X_train:{len(X_train)}\")\n",
    "print (f\"X_test:{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF6kV-EsP5vS"
   },
   "source": [
    "# Preprocessing\n",
    "### Concatenate\n",
    "\n",
    "To split the dataset with 80-10-10 ratio, we'll first concatenate train and test datasets to create one big dataset.\n",
    "\n",
    "ğŸ“Œ Use contenate() function of the NumPy library for this.\n",
    "\n",
    "# Ã–n iÅŸleme\n",
    "### BirleÅŸtir\n",
    "\n",
    "Veri kÃ¼mesini 80-10-10 oranÄ±na bÃ¶lmek iÃ§in Ã¶ncelikle eÄŸitim ve test veri kÃ¼melerini birleÅŸtirip bÃ¼yÃ¼k bir veri kÃ¼mesi oluÅŸturacaÄŸÄ±z.\n",
    "\n",
    "ğŸ“Œ Bunun iÃ§in NumPy kÃ¼tÃ¼phanesinin contenate() fonksiyonunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Whj2C-SlKv2E"
   },
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test and assing it to a variable X\n",
    "X= np.concatenate((X_train,X_test),axis =0)\n",
    "\n",
    "# Concatenate y_train and y_test and assing it to a variable y\n",
    "y= np.concatenate((y_train,y_test),axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZObXVorUxoGK"
   },
   "source": [
    "###Padding\n",
    "\n",
    "Since all reviews are at different lengths, we'll use padding to make all of them same length.\n",
    "\n",
    "ğŸ“Œ Use preprocessing.sequence.pad_sequences() function for this.\n",
    "\n",
    "###Dolgu malzemesi\n",
    "\n",
    "TÃ¼m incelemeler farklÄ± uzunluklarda olduÄŸundan, hepsini aynÄ± uzunlukta yapmak iÃ§in dolgu kullanacaÄŸÄ±z.\n",
    "\n",
    "ğŸ“Œ Bunun iÃ§in preprocessing.sequence.pad_sequences() fonksiyonunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "T8mlvy8xKu7-"
   },
   "outputs": [],
   "source": [
    "# Pad all reviews in the X dataset to the length maxlen=1024\n",
    "X=tf.keras.preprocessing.sequence.pad_sequences(X,maxlen=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rZILMK5_-e4"
   },
   "source": [
    "### Splitting\n",
    "\n",
    "Now, split X and y into train, validation and test dataset and assign those to corresponding values.\n",
    "\n",
    "ğŸ“Œ You can use list slicing methods for this.\n",
    "\n",
    "ğŸ“Œ For this dataset, a 80-10-10 split corresponds to 40000 - 10000 - 10000 number of samples relatively.\n",
    "\n",
    "\n",
    "### BÃ¶lme\n",
    "\n",
    "Åimdi X ve y'yi eÄŸitim, doÄŸrulama ve test veri kÃ¼mesine bÃ¶lÃ¼n ve bunlarÄ± karÅŸÄ±lÄ±k gelen deÄŸerlere atayÄ±n.\n",
    "\n",
    "ğŸ“ŒBunun iÃ§in liste dilimleme yÃ¶ntemlerini kullanabilirsiniz.\n",
    "\n",
    "ğŸ“ŒBu veri seti iÃ§in 80-10-10'luk bir bÃ¶lÃ¼nme gÃ¶receli olarak 40000 - 10000 - 10000 Ã¶rnek sayÄ±sÄ±na karÅŸÄ±lÄ±k gelmektedir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ru_A80XWPr05"
   },
   "outputs": [],
   "source": [
    "# Create the training datasets\n",
    "X_train = X[:40000]\n",
    "y_train = y[:40000]\n",
    "\n",
    "\n",
    "# Create the validation datasets\n",
    "X_val = X[40000:45000]\n",
    "y_val = y[40000:45000]\n",
    "# Create the test datasets\n",
    "X_test = X[45000:50000]\n",
    "y_test = y[45000:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4t0TWEuCs6q"
   },
   "source": [
    "To check if that worked out, print the number of samples in each dataset again.\n",
    "\n",
    "ğŸ“Œ Use f-strings for this.\n",
    "\n",
    "\n",
    "Bunun iÅŸe yarayÄ±p yaramadÄ±ÄŸÄ±nÄ± kontrol etmek iÃ§in her veri kÃ¼mesindeki Ã¶rnek sayÄ±sÄ±nÄ± tekrar yazdÄ±rÄ±n.\n",
    "\n",
    "ğŸ“ŒBunun iÃ§in f-stringleri kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yhRLn4stTA4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:40000\n",
      "y_train:40000\n",
      "X_val:5000\n",
      "y_val:5000\n",
      "X_test:5000\n",
      "y_test:5000\n"
     ]
    }
   ],
   "source": [
    "# Print the number of samples\n",
    "print (f\"X_train:{len(X_train)}\")\n",
    "print (f\"y_train:{len(y_train)}\")\n",
    "\n",
    "print (f\"X_val:{len(X_val)}\")\n",
    "print (f\"y_val:{len(y_val)}\")\n",
    "\n",
    "\n",
    "print (f\"X_test:{len(X_test)}\")\n",
    "print (f\"y_test:{len(y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDCMa-o8ESLy"
   },
   "source": [
    "## Constructing the neural network\n",
    "\n",
    "That was it for the preprocessing of the data! \n",
    "\n",
    "Now we can create our model. First, we start by creating a model object using the Sequential API of Keras.\n",
    "\n",
    "ğŸ“Œ Use tf.keras.Sequential() to create a model object\n",
    "\n",
    "## Sinir aÄŸÄ±nÄ±n oluÅŸturulmasÄ±\n",
    "\n",
    "Verilerin Ã¶n iÅŸlenmesi bu kadardÄ±!\n",
    "\n",
    "ArtÄ±k modelimizi oluÅŸturabiliriz. Ã–ncelikle Keras'Ä±n Sequential API'sini kullanarak bir model nesnesi oluÅŸturarak baÅŸlÄ±yoruz.\n",
    "\n",
    "ğŸ“Œ Bir model nesnesi oluÅŸturmak iÃ§in tf.keras.Sequential() Ã¶ÄŸesini kullanÄ±n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-lodLU07jdzm"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lh7_MzgFhIf"
   },
   "source": [
    "### Embedding Layer\n",
    "\n",
    "For the first layer, we add an embedding layer.\n",
    "\n",
    "ğŸ“Œ Use tf.keras.layers.Embedding() for the embedding layer.\n",
    "\n",
    "ğŸ“Œ Use .add() method of the object to add the layer.\n",
    "\n",
    "### GÃ¶mme KatmanÄ±\n",
    "â€‹\n",
    "Ä°lk katman iÃ§in bir gÃ¶mme katmanÄ± ekliyoruz.\n",
    "â€‹\n",
    "ğŸ“Œ GÃ¶mme katmanÄ± iÃ§in tf.keras.layers.Embedding() iÅŸlevini kullanÄ±n.\n",
    "â€‹\n",
    "ğŸ“Œ KatmanÄ± eklemek iÃ§in nesnenin .add() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "41CLMa1Epasa"
   },
   "outputs": [],
   "source": [
    "# Add an embedding layer and a dropout\n",
    "model.add(tf.keras.layers.Embedding(input_dim=10000,output_dim=256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpeVhPpEG3u9"
   },
   "source": [
    "Then, we add a LSTM layer and a dense layer; each with a dropout.\n",
    "\n",
    "ğŸ“Œ Use tf.keras.layers.LSTM() and tf.keras.layers.Dense() to create the layers.\n",
    "\n",
    "ğŸ“Œ Use .add() method of the object to add the layer.\n",
    "\n",
    "\n",
    "Daha sonra bir LSTM katmanÄ± ve bir yoÄŸun katman ekliyoruz; her biri bir okulu bÄ±rakmÄ±ÅŸ durumda.\n",
    "\n",
    "ğŸ“Œ KatmanlarÄ± oluÅŸturmak iÃ§in tf.keras.layers.LSTM() ve tf.keras.layers.Dense()'yi kullanÄ±n.\n",
    "\n",
    "ğŸ“Œ KatmanÄ± eklemek iÃ§in nesnenin .add() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ntaW1KWrpngU"
   },
   "outputs": [],
   "source": [
    "# Add a LSTM layer with dropout\n",
    "model.add(tf.keras.layers.LSTM(256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n",
    "\n",
    "\n",
    "# Add a Dense layer with dropout\n",
    "model.add(tf.keras.layers.Dense(128,activation= \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWRJxTGHhaI"
   },
   "source": [
    "### Output layer\n",
    "\n",
    "As the last part of our neural network, we add the output layer. The number of nodes will be one since we are making binary classification. We'll use the sigmoid activation function in the output layer.\n",
    "\n",
    "ğŸ“Œ Use tf.keras.layers.Dense() to create the layer.\n",
    "\n",
    "ğŸ“Œ Use .add() method of the object to add the layer.\n",
    "\n",
    "### Ã‡Ä±kÄ±ÅŸ katmanÄ±\n",
    "\n",
    "Sinir aÄŸÄ±mÄ±zÄ±n son parÃ§asÄ± olarak Ã§Ä±ktÄ± katmanÄ±nÄ± ekliyoruz. Ä°kili sÄ±nÄ±flandÄ±rma yaptÄ±ÄŸÄ±mÄ±z iÃ§in dÃ¼ÄŸÃ¼m sayÄ±sÄ± bir olacaktÄ±r. Ã‡Ä±kÄ±ÅŸ katmanÄ±nda sigmoid aktivasyon fonksiyonunu kullanacaÄŸÄ±z.\n",
    "\n",
    "ğŸ“Œ KatmanÄ± oluÅŸturmak iÃ§in tf.keras.layers.Dense() Ã¶ÄŸesini kullanÄ±n.\n",
    "\n",
    "ğŸ“Œ KatmanÄ± eklemek iÃ§in nesnenin .add() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1ufBdJmBs_T-"
   },
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "model.add(tf.keras.layers.Dense(1,activation =\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7EI9LX1I522"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "Now we have the structure of our model. To configure the model for training, we'll use the *.compile()* method. Inside the compile method, we have to define the following:\n",
    "*   \"Adam\" for optimizer\n",
    "*   \"Binary Crossentropy\" for the loss function\n",
    "\n",
    "\n",
    "ğŸ“Œ Construct the model with the .compile() method.\n",
    "\n",
    "Optimize Edici\n",
    "ArtÄ±k modelimizin yapÄ±sÄ±na sahibiz. Modeli eÄŸitim amacÄ±yla yapÄ±landÄ±rmak iÃ§in .compile() yÃ¶ntemini kullanacaÄŸÄ±z. Derleme yÃ¶nteminin iÃ§inde aÅŸaÄŸÄ±dakileri tanÄ±mlamamÄ±z gerekir:\n",
    "\n",
    "Optimize edici iÃ§in \"Adam\"\n",
    "KayÄ±p fonksiyonu iÃ§in \"Ä°kili Krosentropi\"\n",
    "ğŸ“ŒModeli .compile() yÃ¶ntemiyle oluÅŸturun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "bkDRiJNW_Dbu"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpcO1HLZJZtZ"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "It's time to train the model. We'll give the X_train and y_train datasets as the first two arguments. These will be used for training. And with the *validation_data* parameter, we'll give the X_val and y_val as a tuple.\n",
    "\n",
    "ğŸ“Œ Use .fit() method of the model object for the training.\n",
    "\n",
    "## Modeli eÄŸitme\n",
    "\n",
    "Modeli eÄŸitmenin zamanÄ± geldi. Ä°lk iki argÃ¼man olarak X_train ve y_train veri kÃ¼melerini vereceÄŸiz. Bunlar eÄŸitim amaÃ§lÄ± kullanÄ±lacak. Ve *validation_data* parametresi ile X_val ve y_val deÄŸerlerini tuple olarak vereceÄŸiz.\n",
    "\n",
    "ğŸ“Œ EÄŸitim iÃ§in model nesnesinin .fit() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoTfLMTt4RQ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 252/1250 [=====>........................] - ETA: 2:01:14 - loss: 0.6583 - accuracy: 0.5902"
     ]
    }
   ],
   "source": [
    "# Train the model for 5 epochs\n",
    "results= model.fit(X_train,y_train,epochs = 5,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEx98AYLJwhl"
   },
   "source": [
    "### Visualize the results\n",
    "\n",
    "After the model is trained, we can create a graph to visualize the change of loss over time. Results are held in:\n",
    "* results.history[\"loss\"]\n",
    "* results.history[\"val_loss\"]\n",
    "\n",
    "ğŸ“Œ Use plt.show() to display the graph.\n",
    "\n",
    "\n",
    "### SonuÃ§larÄ± gÃ¶rselleÅŸtirin\n",
    "â€‹\n",
    "Model eÄŸitildikten sonra zaman iÃ§indeki kaybÄ±n deÄŸiÅŸimini gÃ¶rselleÅŸtirmek iÃ§in bir grafik oluÅŸturabiliriz. SonuÃ§lar ÅŸurada tutulur:\n",
    "* sonuÃ§lar.geÃ§miÅŸ[\"kayÄ±p\"]\n",
    "* sonuÃ§lar.geÃ§miÅŸ[\"val_loss\"]\n",
    "â€‹\n",
    "ğŸ“Œ GrafiÄŸi gÃ¶rÃ¼ntÃ¼lemek iÃ§in plt.show() iÅŸlevini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDw7KpHct81z"
   },
   "outputs": [],
   "source": [
    "# Plot the the training loss\n",
    "plt.plot(results.history[\"loss\"],label =\"Train\")\n",
    "\n",
    "# Plot the the validation loss\n",
    "plt.plot(results.history[\"val_loss\",label=\"Validation\"])\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Put legend table\n",
    "plt.lenged()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4f-9V6pKHfE"
   },
   "source": [
    "Now, do the same thing for accuracy.\n",
    "\n",
    "ğŸ“Œ Accuracy scores can be found in:\n",
    "* results.history[\"accuracy\"]\n",
    "* results.history[\"val_accuracy\"]\n",
    "\n",
    "\n",
    "\n",
    "Åimdi doÄŸruluk iÃ§in aynÄ± ÅŸeyi yapÄ±n.\n",
    "\n",
    "ğŸ“Œ DoÄŸruluk puanlarÄ±nÄ± ÅŸurada bulabilirsiniz:\n",
    "* sonuÃ§lar.geÃ§miÅŸ[\"doÄŸruluk\"]\n",
    "* sonuÃ§lar.geÃ§miÅŸ[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LUeUQAn_CkD"
   },
   "outputs": [],
   "source": [
    "# Plot the the training accuracy\n",
    "plt.plot(results.history[\"accuracy\"],label=\"Train\")\n",
    "\n",
    "# Plot the the validation accuracy\n",
    "plt.plot(results.history[\"val_accuracy\"],label=\"Validation\")\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# Put legend table\n",
    "plt.lenged()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnz14s_zKSq8"
   },
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Let's use the test dataset that we created to evaluate the performance of the model.\n",
    "\n",
    "ğŸ“Œ Use test_on_batch() method with test dataset as parameter.\n",
    "\n",
    "\n",
    "\n",
    "## Performans deÄŸerlendirmesi\n",
    "\n",
    "Modelin performansÄ±nÄ± deÄŸerlendirmek iÃ§in oluÅŸturduÄŸumuz test veri setini kullanalÄ±m.\n",
    "\n",
    "ğŸ“Œ Test veri kÃ¼mesini parametre olarak kullanarak test_on_batch() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grHvXCZY_JVT"
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJH4551KWWe"
   },
   "source": [
    "### Try a prediction\n",
    "\n",
    "Next, we take a sample and make a prediction on it.\n",
    "\n",
    "ğŸ“Œ Reshape the review to (1, 1024).\n",
    "\n",
    "ğŸ“Œ Use the .prediction() method of the model object.\n",
    "\n",
    "\n",
    "### Bir tahmin deneyin\n",
    "\n",
    "Daha sonra bir Ã¶rnek alÄ±yoruz ve onun Ã¼zerinde bir tahmin yapÄ±yoruz.\n",
    "\n",
    "ğŸ“Œ Ä°ncelemeyi (1, 1024) olarak yeniden ÅŸekillendirin.\n",
    "\n",
    "ğŸ“Œ Model nesnesinin .prediction() yÃ¶ntemini kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vda8VhZh_LiK"
   },
   "outputs": [],
   "source": [
    "# Make prediction on the reshaped sample\n",
    "prediction_result =model.predict(X_test[789].reshape(1,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"label: {y_test[789]} -- Prediction {prediction_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Guided_Project_3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
