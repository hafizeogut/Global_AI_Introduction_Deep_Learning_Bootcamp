{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYmYuGSWFs3-"
   },
   "source": [
    "# Building a Recurrent Neural Network\n",
    "\n",
    "## Sentiment Analysis\n",
    "In this project, we will build a Long Short-term Memory (LSTM) neural network to solve a binary sentiment analysis problem.\n",
    "\n",
    "For this, we'll use the “IMDB Movie Review Dataset\" available on Keras. It includes 50000 highly polarized movie reviews categorized as positive or negative.\n",
    "\n",
    "\n",
    "\n",
    "# Tekrarlayan Bir Sinir Ağı Oluşturmak\n",
    "\n",
    "## Duygu Analizi\n",
    "Bu projede, ikili duygu analizi problemini çözmek için Uzun Kısa Süreli Bellek (LSTM) sinir ağı oluşturacağız.\n",
    "\n",
    "Bunun için Keras'ta bulunan “IMDB Film İnceleme Veri Kümesini” kullanacağız. Olumlu veya olumsuz olarak kategorize edilmiş 50.000 yüksek derecede kutuplaşmış film incelemesini içerir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQQ7xy4lzfsw"
   },
   "source": [
    "## Importing the required libraries\n",
    "We'll start with importing required libraries.\n",
    "\n",
    "📌 Use the keyword \"import\".\n",
    "\n",
    "\n",
    "## Gerekli kitaplıkları içe aktarma\n",
    "Gerekli kütüphaneleri içe aktararak başlayacağız.\n",
    "\n",
    "📌 \"İçe aktar\" anahtar kelimesini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "b__mue-XGPZ9"
   },
   "outputs": [],
   "source": [
    "# Import TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Import NumPy and Matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u0b5YzGHP3qs"
   },
   "source": [
    "## Dataset\n",
    "Let's download the IMDB dataset which is included in Keras, and assign it to the corresponding variables *X_train*, *y_train*, *X_test*, and *y_test*. We want to include the most frequently used 10000 words, so we specify 10000 for the num_words parameter.\n",
    "\n",
    "📌 Use the datasets.imdb.load_data() function of the Keras.\n",
    "\n",
    "## Veri Kümesi\n",
    "Keras'ın içerdiği IMDB veri setini indirelim ve onu ilgili *X_train*, *y_train*, *X_test* ve *y_test* değişkenlerine atayalım. En sık kullanılan 10000 kelimeyi dahil etmek istiyoruz bu yüzden num_words parametresi için 10000 belirtiyoruz.\n",
    "\n",
    "📌 Keras'ın datasets.imdb.load_data() fonksiyonunu kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "1WLgLQxGGDz8"
   },
   "outputs": [],
   "source": [
    "# Download the IMDB dataset included in Keras\n",
    "# Set the parameter num_words to 10000\n",
    "(X_train,y_train),(X_test,y_test) = tf.keras.datasets.imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUPnNCgC0mHm"
   },
   "source": [
    "Before we move on, we can print a single sample to see what the data looks like.\n",
    "\n",
    "📌 Use the print() function for this.\n",
    "\n",
    "Devam etmeden önce verilerin neye benzediğini görmek için tek bir örnek yazdırabiliriz.\n",
    "\n",
    "📌 Bunun için print() fonksiyonunu kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "1spB5eY9xh-B"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# Print a sample\n",
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VKkhznIa8hIw"
   },
   "source": [
    "Then, we print the the number of samples in the X_train and X_test datasets to see how the dataset is distributed.\n",
    "\n",
    "📌 Use f-strings for this.\n",
    "\n",
    "Daha sonra veri setinin nasıl dağıldığını görmek için X_train ve X_test veri setlerindeki örnek sayısını yazdırıyoruz.\n",
    "\n",
    "📌Bunun için f-stringleri kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "skzb2oTCdV-c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:25000\n",
      "X_test:25000\n"
     ]
    }
   ],
   "source": [
    "# Print the number of samples\n",
    "print (f\"X_train:{len(X_train)}\")\n",
    "print (f\"X_test:{len(X_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lF6kV-EsP5vS"
   },
   "source": [
    "# Preprocessing\n",
    "### Concatenate\n",
    "\n",
    "To split the dataset with 80-10-10 ratio, we'll first concatenate train and test datasets to create one big dataset.\n",
    "\n",
    "📌 Use contenate() function of the NumPy library for this.\n",
    "\n",
    "# Ön işleme\n",
    "### Birleştir\n",
    "\n",
    "Veri kümesini 80-10-10 oranına bölmek için öncelikle eğitim ve test veri kümelerini birleştirip büyük bir veri kümesi oluşturacağız.\n",
    "\n",
    "📌 Bunun için NumPy kütüphanesinin contenate() fonksiyonunu kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Whj2C-SlKv2E"
   },
   "outputs": [],
   "source": [
    "# Concatenate X_train and X_test and assing it to a variable X\n",
    "X= np.concatenate((X_train,X_test),axis =0)\n",
    "\n",
    "# Concatenate y_train and y_test and assing it to a variable y\n",
    "y= np.concatenate((y_train,y_test),axis =0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZObXVorUxoGK"
   },
   "source": [
    "###Padding\n",
    "\n",
    "Since all reviews are at different lengths, we'll use padding to make all of them same length.\n",
    "\n",
    "📌 Use preprocessing.sequence.pad_sequences() function for this.\n",
    "\n",
    "###Dolgu malzemesi\n",
    "\n",
    "Tüm incelemeler farklı uzunluklarda olduğundan, hepsini aynı uzunlukta yapmak için dolgu kullanacağız.\n",
    "\n",
    "📌 Bunun için preprocessing.sequence.pad_sequences() fonksiyonunu kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "T8mlvy8xKu7-"
   },
   "outputs": [],
   "source": [
    "# Pad all reviews in the X dataset to the length maxlen=1024\n",
    "X=tf.keras.preprocessing.sequence.pad_sequences(X,maxlen=1024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2rZILMK5_-e4"
   },
   "source": [
    "### Splitting\n",
    "\n",
    "Now, split X and y into train, validation and test dataset and assign those to corresponding values.\n",
    "\n",
    "📌 You can use list slicing methods for this.\n",
    "\n",
    "📌 For this dataset, a 80-10-10 split corresponds to 40000 - 10000 - 10000 number of samples relatively.\n",
    "\n",
    "\n",
    "### Bölme\n",
    "\n",
    "Şimdi X ve y'yi eğitim, doğrulama ve test veri kümesine bölün ve bunları karşılık gelen değerlere atayın.\n",
    "\n",
    "📌Bunun için liste dilimleme yöntemlerini kullanabilirsiniz.\n",
    "\n",
    "📌Bu veri seti için 80-10-10'luk bir bölünme göreceli olarak 40000 - 10000 - 10000 örnek sayısına karşılık gelmektedir.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Ru_A80XWPr05"
   },
   "outputs": [],
   "source": [
    "# Create the training datasets\n",
    "X_train = X[:40000]\n",
    "y_train = y[:40000]\n",
    "\n",
    "\n",
    "# Create the validation datasets\n",
    "X_val = X[40000:45000]\n",
    "y_val = y[40000:45000]\n",
    "# Create the test datasets\n",
    "X_test = X[45000:50000]\n",
    "y_test = y[45000:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4t0TWEuCs6q"
   },
   "source": [
    "To check if that worked out, print the number of samples in each dataset again.\n",
    "\n",
    "📌 Use f-strings for this.\n",
    "\n",
    "\n",
    "Bunun işe yarayıp yaramadığını kontrol etmek için her veri kümesindeki örnek sayısını tekrar yazdırın.\n",
    "\n",
    "📌Bunun için f-stringleri kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "yhRLn4stTA4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:40000\n",
      "y_train:40000\n",
      "X_val:5000\n",
      "y_val:5000\n",
      "X_test:5000\n",
      "y_test:5000\n"
     ]
    }
   ],
   "source": [
    "# Print the number of samples\n",
    "print (f\"X_train:{len(X_train)}\")\n",
    "print (f\"y_train:{len(y_train)}\")\n",
    "\n",
    "print (f\"X_val:{len(X_val)}\")\n",
    "print (f\"y_val:{len(y_val)}\")\n",
    "\n",
    "\n",
    "print (f\"X_test:{len(X_test)}\")\n",
    "print (f\"y_test:{len(y_test)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDCMa-o8ESLy"
   },
   "source": [
    "## Constructing the neural network\n",
    "\n",
    "That was it for the preprocessing of the data! \n",
    "\n",
    "Now we can create our model. First, we start by creating a model object using the Sequential API of Keras.\n",
    "\n",
    "📌 Use tf.keras.Sequential() to create a model object\n",
    "\n",
    "## Sinir ağının oluşturulması\n",
    "\n",
    "Verilerin ön işlenmesi bu kadardı!\n",
    "\n",
    "Artık modelimizi oluşturabiliriz. Öncelikle Keras'ın Sequential API'sini kullanarak bir model nesnesi oluşturarak başlıyoruz.\n",
    "\n",
    "📌 Bir model nesnesi oluşturmak için tf.keras.Sequential() öğesini kullanın"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "-lodLU07jdzm"
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lh7_MzgFhIf"
   },
   "source": [
    "### Embedding Layer\n",
    "\n",
    "For the first layer, we add an embedding layer.\n",
    "\n",
    "📌 Use tf.keras.layers.Embedding() for the embedding layer.\n",
    "\n",
    "📌 Use .add() method of the object to add the layer.\n",
    "\n",
    "### Gömme Katmanı\n",
    "​\n",
    "İlk katman için bir gömme katmanı ekliyoruz.\n",
    "​\n",
    "📌 Gömme katmanı için tf.keras.layers.Embedding() işlevini kullanın.\n",
    "​\n",
    "📌 Katmanı eklemek için nesnenin .add() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "41CLMa1Epasa"
   },
   "outputs": [],
   "source": [
    "# Add an embedding layer and a dropout\n",
    "model.add(tf.keras.layers.Embedding(input_dim=10000,output_dim=256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpeVhPpEG3u9"
   },
   "source": [
    "Then, we add a LSTM layer and a dense layer; each with a dropout.\n",
    "\n",
    "📌 Use tf.keras.layers.LSTM() and tf.keras.layers.Dense() to create the layers.\n",
    "\n",
    "📌 Use .add() method of the object to add the layer.\n",
    "\n",
    "\n",
    "Daha sonra bir LSTM katmanı ve bir yoğun katman ekliyoruz; her biri bir okulu bırakmış durumda.\n",
    "\n",
    "📌 Katmanları oluşturmak için tf.keras.layers.LSTM() ve tf.keras.layers.Dense()'yi kullanın.\n",
    "\n",
    "📌 Katmanı eklemek için nesnenin .add() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ntaW1KWrpngU"
   },
   "outputs": [],
   "source": [
    "# Add a LSTM layer with dropout\n",
    "model.add(tf.keras.layers.LSTM(256))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n",
    "\n",
    "\n",
    "# Add a Dense layer with dropout\n",
    "model.add(tf.keras.layers.Dense(128,activation= \"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.7))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTWRJxTGHhaI"
   },
   "source": [
    "### Output layer\n",
    "\n",
    "As the last part of our neural network, we add the output layer. The number of nodes will be one since we are making binary classification. We'll use the sigmoid activation function in the output layer.\n",
    "\n",
    "📌 Use tf.keras.layers.Dense() to create the layer.\n",
    "\n",
    "📌 Use .add() method of the object to add the layer.\n",
    "\n",
    "### Çıkış katmanı\n",
    "\n",
    "Sinir ağımızın son parçası olarak çıktı katmanını ekliyoruz. İkili sınıflandırma yaptığımız için düğüm sayısı bir olacaktır. Çıkış katmanında sigmoid aktivasyon fonksiyonunu kullanacağız.\n",
    "\n",
    "📌 Katmanı oluşturmak için tf.keras.layers.Dense() öğesini kullanın.\n",
    "\n",
    "📌 Katmanı eklemek için nesnenin .add() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "1ufBdJmBs_T-"
   },
   "outputs": [],
   "source": [
    "# Add the output layer\n",
    "model.add(tf.keras.layers.Dense(1,activation =\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7EI9LX1I522"
   },
   "source": [
    "### Optimizer\n",
    "\n",
    "Now we have the structure of our model. To configure the model for training, we'll use the *.compile()* method. Inside the compile method, we have to define the following:\n",
    "*   \"Adam\" for optimizer\n",
    "*   \"Binary Crossentropy\" for the loss function\n",
    "\n",
    "\n",
    "📌 Construct the model with the .compile() method.\n",
    "\n",
    "Optimize Edici\n",
    "Artık modelimizin yapısına sahibiz. Modeli eğitim amacıyla yapılandırmak için .compile() yöntemini kullanacağız. Derleme yönteminin içinde aşağıdakileri tanımlamamız gerekir:\n",
    "\n",
    "Optimize edici için \"Adam\"\n",
    "Kayıp fonksiyonu için \"İkili Krosentropi\"\n",
    "📌Modeli .compile() yöntemiyle oluşturun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "bkDRiJNW_Dbu"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\",metrics =[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vpcO1HLZJZtZ"
   },
   "source": [
    "## Training the model\n",
    "\n",
    "It's time to train the model. We'll give the X_train and y_train datasets as the first two arguments. These will be used for training. And with the *validation_data* parameter, we'll give the X_val and y_val as a tuple.\n",
    "\n",
    "📌 Use .fit() method of the model object for the training.\n",
    "\n",
    "## Modeli eğitme\n",
    "\n",
    "Modeli eğitmenin zamanı geldi. İlk iki argüman olarak X_train ve y_train veri kümelerini vereceğiz. Bunlar eğitim amaçlı kullanılacak. Ve *validation_data* parametresi ile X_val ve y_val değerlerini tuple olarak vereceğiz.\n",
    "\n",
    "📌 Eğitim için model nesnesinin .fit() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PoTfLMTt4RQ1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      " 252/1250 [=====>........................] - ETA: 2:01:14 - loss: 0.6583 - accuracy: 0.5902"
     ]
    }
   ],
   "source": [
    "# Train the model for 5 epochs\n",
    "results= model.fit(X_train,y_train,epochs = 5,validation_data=(X_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEx98AYLJwhl"
   },
   "source": [
    "### Visualize the results\n",
    "\n",
    "After the model is trained, we can create a graph to visualize the change of loss over time. Results are held in:\n",
    "* results.history[\"loss\"]\n",
    "* results.history[\"val_loss\"]\n",
    "\n",
    "📌 Use plt.show() to display the graph.\n",
    "\n",
    "\n",
    "### Sonuçları görselleştirin\n",
    "​\n",
    "Model eğitildikten sonra zaman içindeki kaybın değişimini görselleştirmek için bir grafik oluşturabiliriz. Sonuçlar şurada tutulur:\n",
    "* sonuçlar.geçmiş[\"kayıp\"]\n",
    "* sonuçlar.geçmiş[\"val_loss\"]\n",
    "​\n",
    "📌 Grafiği görüntülemek için plt.show() işlevini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDw7KpHct81z"
   },
   "outputs": [],
   "source": [
    "# Plot the the training loss\n",
    "plt.plot(results.history[\"loss\"],label =\"Train\")\n",
    "\n",
    "# Plot the the validation loss\n",
    "plt.plot(results.history[\"val_loss\",label=\"Validation\"])\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "\n",
    "# Put legend table\n",
    "plt.lenged()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x4f-9V6pKHfE"
   },
   "source": [
    "Now, do the same thing for accuracy.\n",
    "\n",
    "📌 Accuracy scores can be found in:\n",
    "* results.history[\"accuracy\"]\n",
    "* results.history[\"val_accuracy\"]\n",
    "\n",
    "\n",
    "\n",
    "Şimdi doğruluk için aynı şeyi yapın.\n",
    "\n",
    "📌 Doğruluk puanlarını şurada bulabilirsiniz:\n",
    "* sonuçlar.geçmiş[\"doğruluk\"]\n",
    "* sonuçlar.geçmiş[\"val_accuracy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7LUeUQAn_CkD"
   },
   "outputs": [],
   "source": [
    "# Plot the the training accuracy\n",
    "plt.plot(results.history[\"accuracy\"],label=\"Train\")\n",
    "\n",
    "# Plot the the validation accuracy\n",
    "plt.plot(results.history[\"val_accuracy\"],label=\"Validation\")\n",
    "\n",
    "# Name the x and y axises\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "# Put legend table\n",
    "plt.lenged()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xnz14s_zKSq8"
   },
   "source": [
    "## Performance evaluation\n",
    "\n",
    "Let's use the test dataset that we created to evaluate the performance of the model.\n",
    "\n",
    "📌 Use test_on_batch() method with test dataset as parameter.\n",
    "\n",
    "\n",
    "\n",
    "## Performans değerlendirmesi\n",
    "\n",
    "Modelin performansını değerlendirmek için oluşturduğumuz test veri setini kullanalım.\n",
    "\n",
    "📌 Test veri kümesini parametre olarak kullanarak test_on_batch() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "grHvXCZY_JVT"
   },
   "outputs": [],
   "source": [
    "# Evaluate the performance\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MOJH4551KWWe"
   },
   "source": [
    "### Try a prediction\n",
    "\n",
    "Next, we take a sample and make a prediction on it.\n",
    "\n",
    "📌 Reshape the review to (1, 1024).\n",
    "\n",
    "📌 Use the .prediction() method of the model object.\n",
    "\n",
    "\n",
    "### Bir tahmin deneyin\n",
    "\n",
    "Daha sonra bir örnek alıyoruz ve onun üzerinde bir tahmin yapıyoruz.\n",
    "\n",
    "📌 İncelemeyi (1, 1024) olarak yeniden şekillendirin.\n",
    "\n",
    "📌 Model nesnesinin .prediction() yöntemini kullanın."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vda8VhZh_LiK"
   },
   "outputs": [],
   "source": [
    "# Make prediction on the reshaped sample\n",
    "prediction_result =model.predict(X_test[789].reshape(1,1024))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"label: {y_test[789]} -- Prediction {prediction_result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Guided_Project_3.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
